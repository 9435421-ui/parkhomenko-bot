"""
RAG-—Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –±–∞–∑–æ–π –∑–Ω–∞–Ω–∏–π
"""
import os
import re
from typing import List, Dict, Optional
import aiofiles


class KnowledgeBase:
    """RAG-—Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π"""
    
    def __init__(self, docs_dir: str = "docs"):
        self.docs_dir = docs_dir
        self.documents: List[Dict[str, str]] = []
        self.indexed = False
    
    async def index_documents(self):
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –≤—Å–µ—Ö .md —Ñ–∞–π–ª–æ–≤ –∏–∑ –ø–∞–ø–∫–∏ docs"""
        if not os.path.exists(self.docs_dir):
            print(f"‚ö†Ô∏è –ü–∞–ø–∫–∞ {self.docs_dir} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
            return
        
        # –ò—Å–∫–ª—é—á–∞–µ–º —Å–∏—Å—Ç–µ–º–Ω—ã–µ –ø–∞–ø–∫–∏
        exclude_dirs = {'knowledge_base', '__pycache__', '.git', 'backups', 'migrations', 'mini_app', 'uploads'}
        
        document_count = 0
        
        for root, dirs, files in os.walk(self.docs_dir):
            # –ò—Å–∫–ª—é—á–∞–µ–º —Å–∏—Å—Ç–µ–º–Ω—ã–µ –ø–∞–ø–∫–∏ –∏–∑ –æ–±—Ö–æ–¥–∞
            dirs[:] = [d for d in dirs if d not in exclude_dirs]
            
            for filename in files:
                if filename.endswith(('.md', '.txt')):
                    filepath = os.path.join(root, filename)
                    try:
                        async with aiofiles.open(filepath, 'r', encoding='utf-8') as f:
                            content = await f.read()
                            
                            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π –ø—É—Ç—å –¥–ª—è –ª—É—á—à–µ–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏
                            relative_path = os.path.relpath(filepath, self.docs_dir)
                            
                            self.documents.append({
                                'filename': relative_path,
                                'content': content,
                                'path': filepath
                            })
                            document_count += 1
                            
                    except Exception as e:
                        print(f"‚ùå –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è {filename}: {e}")
        
        self.indexed = True
        print(f"‚úÖ –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–∞: {document_count} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
        return document_count
    
    async def get_context(
        self,
        query: str,
        max_chunks: int = 3,
        context_size: int = 800
    ) -> str:
        """
        –ü–æ–ª—É—á–∏—Ç—å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø–æ –∑–∞–ø—Ä–æ—Å—É
        
        Args:
            query: –ó–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            max_chunks: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤
            context_size: –†–∞–∑–º–µ—Ä –∫–∞–∂–¥–æ–≥–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ –≤ —Å–∏–º–≤–æ–ª–∞—Ö
        
        Returns:
            str: –†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π
        """
        if not self.indexed:
            await self.index_documents()
        
        if not self.documents:
            return "–ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –ø—É—Å—Ç–∞."
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞
        query_lower = query.lower()
        keywords = self._extract_keywords(query_lower)
        
        # –û—Ü–µ–Ω–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        scored_docs = []
        for doc in self.documents:
            score = self._calculate_relevance_score(doc['content'], keywords)
            if score > 0:
                scored_docs.append((score, doc))
        
        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
        scored_docs.sort(reverse=True, key=lambda x: x[0])
        
        # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        if not scored_docs:
            return "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –ø–æ –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π."
        
        context_parts = []
        for i, (score, doc) in enumerate(scored_docs[:max_chunks]):
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π —Ñ—Ä–∞–≥–º–µ–Ω—Ç
            snippet = self._extract_relevant_snippet(
                doc['content'],
                keywords,
                context_size
            )
            context_parts.append(
                f"üìÑ –ò–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞ '{doc['filename']}':\n{snippet}"
            )
        
        return "\n\n".join(context_parts)
    
    def _extract_keywords(self, text: str) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        # –£–¥–∞–ª—è–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ –∏ –∏–∑–≤–ª–µ–∫–∞–µ–º –∑–Ω–∞—á–∏–º—ã–µ —Å–ª–æ–≤–∞
        stop_words = {
            '–∫–∞–∫', '—á—Ç–æ', '—ç—Ç–æ', '–≥–¥–µ', '–∫–æ–≥–¥–∞', '–ø–æ—á–µ–º—É', '–º–æ–∂–Ω–æ', '–Ω—É–∂–Ω–æ',
            '—Ö–æ—á—É', '—Ö–æ—Ç–∏–º', '–Ω—É–∂–µ–Ω', '–µ—Å—Ç—å', '–ª–∏', '–∏–ª–∏', '—Ç–∞–∫–∂–µ', '–µ—Å–ª–∏'
        }
        
        words = re.findall(r'\w+', text.lower())
        keywords = [w for w in words if len(w) > 3 and w not in stop_words]
        
        return keywords
    
    def _calculate_relevance_score(
        self,
        content: str,
        keywords: List[str]
    ) -> int:
        """–†–∞—Å—á—ë—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤"""
        content_lower = content.lower()
        
        # –ü—Ä–æ—Å—Ç–æ–π –ø–æ–¥—Å—á—ë—Ç –≤—Ö–æ–∂–¥–µ–Ω–∏–π –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        score = 0
        for keyword in keywords:
            score += content_lower.count(keyword)
        
        return score
    
    def _extract_relevant_snippet(
        self,
        content: str,
        keywords: List[str],
        context_size: int = 800
    ) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–≥–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        content_lower = content.lower()
        
        # –ü–æ–∏—Å–∫ –ø–µ—Ä–≤–æ–≥–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è —Å –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–æ–º
        best_position = -1
        for keyword in keywords:
            pos = content_lower.find(keyword)
            if pos != -1 and (best_position == -1 or pos < best_position):
                best_position = pos
        
        if best_position == -1:
            # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ ‚Äî –±–µ—Ä—ë–º –Ω–∞—á–∞–ª–æ
            snippet = content[:context_size]
            if len(content) > context_size:
                snippet += "..."
            return snippet
        
        # –ë–µ—Ä—ë–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –≤–æ–∫—Ä—É–≥ –Ω–∞–π–¥–µ–Ω–Ω–æ–≥–æ –∫–ª—é—á–µ–≤–æ–≥–æ —Å–ª–æ–≤–∞
        start = max(0, best_position - context_size // 2)
        end = min(len(content), best_position + context_size // 2)
        
        snippet = content[start:end]
        
        if start > 0:
            snippet = "..." + snippet
        if end < len(content):
            snippet = snippet + "..."
        
        return snippet
    
    def get_document_categories(self) -> List[str]:
        """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –∫–∞—Ç–µ–≥–æ—Ä–∏–π –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π)"""
        categories = set()
        
        for doc in self.documents:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–µ—Ä–≤—É—é —á–∞—Å—Ç—å –ø—É—Ç–∏ (–∫–∞—Ç–µ–≥–æ—Ä–∏—é)
            parts = doc['filename'].split(os.sep)
            if len(parts) > 1:
                categories.add(parts[0])
        
        return sorted(list(categories))
    
    def get_documents_by_category(self, category: str) -> List[Dict[str, str]]:
        """–ü–æ–ª—É—á–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç—ã –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏"""
        return [
            doc for doc in self.documents
            if doc['filename'].startswith(category)
        ]


# Singleton instance
kb = KnowledgeBase()


# Alias –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
async def search(query: str, limit: int = 3) -> List[Dict[str, str]]:
    """–£–¥–æ–±–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è-–∞–ª–∏–∞—Å –¥–ª—è get_context"""
    context = await kb.get_context(query, max_chunks=limit)
    return [{'text': context, 'source': 'knowledge_base'}]
